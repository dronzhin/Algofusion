# models/deepseek_ocr2.py

"""
DeepSeek-OCR 2 –º–æ–¥–µ–ª—å –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
–£–ª—É—á—à–µ–Ω–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
"""

from transformers import AutoProcessor, AutoModelForCausalLM
import torch
from PIL import Image
from utils import logger
import time
from typing import Tuple, Optional


class DeepSeekOCR2Model:
    def __init__(self):
        logger.info("‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ DeepSeek-OCR 2...")

        try:
            start_time = time.time()

            # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å (–∏–º—è –º–æ–∂–µ—Ç –æ—Ç–ª–∏—á–∞—Ç—å—Å—è)
            model_name = "deepseek-ai/DeepSeek-OCR-2"
            logger.debug(f"   –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {model_name}")

            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                trust_remote_code=True,
                torch_dtype=torch.float16,
                device_map="auto"
            )

            self.processor = AutoProcessor.from_pretrained(
                model_name,
                trust_remote_code=True
            )

            load_time = time.time() - start_time
            device = next(self.model.parameters()).device

            logger.info(f"‚úÖ DeepSeek-OCR 2 –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ {load_time:.2f} —Å–µ–∫ | –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
            logger.debug(f"   –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏: {sum(p.numel() for p in self.model.parameters()) / 1e9:.1f}B")

        except Exception as e:
            error_msg = str(e)
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ DeepSeek-OCR 2: {error_msg}", exc_info=True)

            # –ü–æ–ø—ã—Ç–∫–∞ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ–≥–æ –∏–º–µ–Ω–∏ –º–æ–¥–µ–ª–∏
            if "404" in error_msg or "not found" in error_msg.lower():
                logger.warning("‚ö†Ô∏è  DeepSeek-OCR 2 –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∞–∫—Ç—É–∞–ª—å–Ω–æ–µ –∏–º—è –Ω–∞ Hugging Face")
                logger.warning("   –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –∏–º–µ–Ω–∞: 'deepseek-ai/DeepSeek-OCR2', 'deepseek-ai/DeepSeek-OCR2-3B'")

            raise

    def infer(self, image: Image.Image, prompt: str = "Extract all text preserving structure", return_confidence: bool = False) -> Tuple[str, Optional[float]]:
        """
        –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è

        Args:
            image: PIL Image –≤ —Ñ–æ—Ä–º–∞—Ç–µ RGB
            prompt: –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –¥–ª—è –º–æ–¥–µ–ª–∏
            return_confidence: –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å –ª–∏ –º–µ—Ç—Ä–∏–∫—É —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏

        Returns:
            (—Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç, —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –∏–ª–∏ None)
        """
        start_time = time.time()

        try:
            logger.debug(f"üìù DeepSeek-OCR 2 –∏–Ω—Ñ–µ—Ä–µ–Ω—Å | –ü—Ä–æ–º–ø—Ç: {prompt[:50]}...")
            logger.debug(f"   –†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {image.size} | –§–æ—Ä–º–∞—Ç: {image.mode}")

            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            inputs = self.processor(images=image, text=prompt, return_tensors="pt").to(self.model.device)

            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º –≤–æ–∑–≤—Ä–∞—Ç–æ–º –ª–æ–≥–∏—Ç–æ–≤
            with torch.no_grad():
                if return_confidence:
                    output = self.model.generate(
                        **inputs,
                        max_new_tokens=2048,
                        output_scores=True,
                        return_dict_in_generate=True
                    )
                    generated_ids = output.sequences[0]
                    scores = output.scores
                else:
                    generated_ids = self.model.generate(**inputs, max_new_tokens=2048)[0]
                    scores = None

            result = self.processor.decode(generated_ids, skip_special_tokens=True)

            # –†–∞—Å—á—ë—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            confidence = None
            if return_confidence:
                from utils import confidence_calculator
                token_conf, _ = confidence_calculator.calculate_from_logits(
                    generated_ids,
                    scores,
                    inputs["input_ids"].shape[1]
                )
                heuristic_conf = confidence_calculator.calculate_heuristic(result, image.size)
                confidence = confidence_calculator.combine_confidences(
                    token_conf,
                    heuristic_conf,
                    has_token_scores=(scores is not None)
                )
                logger.debug(f"   –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: —Ç–æ–∫–µ–Ω–Ω–∞—è={token_conf:.2f}, —ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∞—è={heuristic_conf:.2f}, –∏—Ç–æ–≥–æ–≤–∞—è={confidence:.2f}")

            infer_time = time.time() - start_time
            logger.info(f"‚úÖ DeepSeek-OCR 2 –∑–∞–≤–µ—Ä—à–µ–Ω–∞ | –í—Ä–µ–º—è: {infer_time:.2f} —Å–µ–∫" +
                       (f" | –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2f}" if confidence else ""))
            logger.debug(f"   –†–µ–∑—É–ª—å—Ç–∞—Ç (–ø–µ—Ä–≤—ã–µ 100 —Å–∏–º–≤–æ–ª–æ–≤): {result[:100]}...")

            return result, confidence

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ DeepSeek-OCR 2: {str(e)}", exc_info=True)
            raise