# models/glm_ocr.py

"""
GLM-OCR 0.9B –º–æ–¥–µ–ª—å –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
–õ—ë–≥–∫–∞—è –∏ –±—ã—Å—Ç—Ä–∞—è –º–æ–¥–µ–ª—å –Ω–∞ –±–∞–∑–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã GLM-4V
"""

from transformers import AutoProcessor, AutoModelForImageTextToText
import torch
from PIL import Image
import warnings
from utils import logger
import time
from typing import Tuple, Optional


class GLMOCRModel:
    def __init__(self):
        logger.info("‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ GLM-OCR 0.9B (zai-org)...")

        try:
            start_time = time.time()

            # –ü–æ–¥–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è transformers
            warnings.filterwarnings("ignore", category=FutureWarning)

            # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
            self.model = AutoModelForImageTextToText.from_pretrained(
                "zai-org/GLM-OCR",
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True
            )

            self.processor = AutoProcessor.from_pretrained(
                "zai-org/GLM-OCR",
                trust_remote_code=True
            )

            load_time = time.time() - start_time
            device = next(self.model.parameters()).device

            logger.info(f"‚úÖ GLM-OCR 0.9B –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ {load_time:.2f} —Å–µ–∫ | –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
            logger.debug(f"   –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏: {sum(p.numel() for p in self.model.parameters()) / 1e9:.1f}B")

        except ImportError as e:
            if "AutoModelForImageTextToText" in str(e):
                logger.error(
                    "‚ùå –¢—Ä–µ–±—É–µ—Ç—Å—è transformers >= 4.46.0 –∏–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏–∑ main:\n"
                    "   pip install git+https://github.com/huggingface/transformers.git"
                )
            raise
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ GLM-OCR: {str(e)}", exc_info=True)
            raise

    def infer(self, image: Image.Image, prompt: str = "Text Recognition:", return_confidence: bool = False) -> Tuple[str, Optional[float]]:
        """
        –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è

        Args:
            image: PIL Image –≤ —Ñ–æ—Ä–º–∞—Ç–µ RGB
            prompt: –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –¥–ª—è –º–æ–¥–µ–ª–∏ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è "Text Recognition:")
            return_confidence: –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å –ª–∏ –º–µ—Ç—Ä–∏–∫—É —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏

        Returns:
            (—Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç, —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –∏–ª–∏ None)
        """
        start_time = time.time()

        try:
            logger.debug(f"üìù GLM-OCR –∏–Ω—Ñ–µ—Ä–µ–Ω—Å | –ü—Ä–æ–º–ø—Ç: {prompt[:50]}...")
            logger.debug(f"   –†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {image.size} | –§–æ—Ä–º–∞—Ç: {image.mode}")

            # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç–µ —á–∞—Ç–∞
            messages = [{
                "role": "user",
                "content": [{"type": "image"}, {"type": "text", "text": prompt}]
            }]

            # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —à–∞–±–ª–æ–Ω–∞ —á–∞—Ç–∞ + —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
            inputs = self.processor.apply_chat_template(
                messages,
                tokenize=True,
                add_generation_prompt=True,
                return_dict=True,
                return_tensors="pt",
                images=image
            ).to(self.model.device)

            # –£–±–∏—Ä–∞–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ –ø–æ–ª—è
            inputs.pop("token_type_ids", None)

            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º –≤–æ–∑–≤—Ä–∞—Ç–æ–º –ª–æ–≥–∏—Ç–æ–≤
            with torch.no_grad():
                if return_confidence:
                    output = self.model.generate(
                        **inputs,
                        max_new_tokens=2048,
                        output_scores=True,
                        return_dict_in_generate=True
                    )
                    generated_ids = output.sequences[0]
                    scores = output.scores
                else:
                    generated_ids = self.model.generate(
                        **inputs,
                        max_new_tokens=2048,
                        do_sample=False,
                        temperature=0.0
                    )[0]
                    scores = None

            # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
            output_text = self.processor.decode(
                generated_ids[inputs["input_ids"].shape[1]:],
                skip_special_tokens=True
            )

            result = output_text.strip()

            # –†–∞—Å—á—ë—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            confidence = None
            if return_confidence:
                from utils import confidence_calculator
                token_conf, _ = confidence_calculator.calculate_from_logits(
                    generated_ids,
                    scores,
                    inputs["input_ids"].shape[1]
                )
                heuristic_conf = confidence_calculator.calculate_heuristic(result, image.size)
                confidence = confidence_calculator.combine_confidences(
                    token_conf,
                    heuristic_conf,
                    has_token_scores=(scores is not None)
                )
                logger.debug(f"   –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: —Ç–æ–∫–µ–Ω–Ω–∞—è={token_conf:.2f}, —ç–≤—Ä–∏—Å—Ç–∏—á–µ—Å–∫–∞—è={heuristic_conf:.2f}, –∏—Ç–æ–≥–æ–≤–∞—è={confidence:.2f}")

            infer_time = time.time() - start_time
            logger.info(f"‚úÖ GLM-OCR –∑–∞–≤–µ—Ä—à–µ–Ω–∞ | –í—Ä–µ–º—è: {infer_time:.2f} —Å–µ–∫" +
                       (f" | –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2f}" if confidence else ""))
            logger.debug(f"   –†–µ–∑—É–ª—å—Ç–∞—Ç (–ø–µ—Ä–≤—ã–µ 100 —Å–∏–º–≤–æ–ª–æ–≤): {result[:100]}...")

            return result, confidence

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ GLM-OCR: {str(e)}", exc_info=True)
            raise